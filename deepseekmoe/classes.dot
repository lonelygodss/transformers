digraph "classes" {
rankdir=BT
charset="utf-8"
"modeling_deepseek.AddAuxiliaryLoss" [color="black", fontcolor="black", label=<{AddAuxiliaryLoss|<br ALIGN="LEFT"/>|backward(ctx, grad_output)<br ALIGN="LEFT"/>forward(ctx, x, loss)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekAttention" [color="black", fontcolor="black", label=<{DeepseekAttention|attention_dropout<br ALIGN="LEFT"/>config<br ALIGN="LEFT"/>head_dim<br ALIGN="LEFT"/>hidden_size<br ALIGN="LEFT"/>is_causal : bool<br ALIGN="LEFT"/>k_proj : Linear<br ALIGN="LEFT"/>layer_idx : Optional[int]<br ALIGN="LEFT"/>max_position_embeddings<br ALIGN="LEFT"/>num_heads<br ALIGN="LEFT"/>num_key_value_groups<br ALIGN="LEFT"/>num_key_value_heads<br ALIGN="LEFT"/>o_proj : Linear<br ALIGN="LEFT"/>q_proj : Linear<br ALIGN="LEFT"/>rope_theta<br ALIGN="LEFT"/>rotary_emb<br ALIGN="LEFT"/>v_proj : Linear<br ALIGN="LEFT"/>|forward(hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.LongTensor], past_key_value: Optional[Cache], output_attentions: bool, use_cache: bool): Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekDecoderLayer" [color="black", fontcolor="black", label=<{DeepseekDecoderLayer|hidden_size<br ALIGN="LEFT"/>input_layernorm<br ALIGN="LEFT"/>mlp<br ALIGN="LEFT"/>post_attention_layernorm<br ALIGN="LEFT"/>self_attn<br ALIGN="LEFT"/>|forward(hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.LongTensor], past_key_value: Optional[Tuple[torch.Tensor]], output_attentions: Optional[bool], use_cache: Optional[bool]): Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekDynamicNTKScalingRotaryEmbedding" [color="black", fontcolor="black", label=<{DeepseekDynamicNTKScalingRotaryEmbedding|max_seq_len_cached<br ALIGN="LEFT"/>scaling_factor : float<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekFlashAttention2" [color="black", fontcolor="black", label=<{DeepseekFlashAttention2|<br ALIGN="LEFT"/>|forward(hidden_states: torch.Tensor, attention_mask: Optional[torch.LongTensor], position_ids: Optional[torch.LongTensor], past_key_value: Optional[Cache], output_attentions: bool, use_cache: bool): Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekForCausalLM" [color="black", fontcolor="black", label=<{DeepseekForCausalLM|lm_head : Linear<br ALIGN="LEFT"/>model<br ALIGN="LEFT"/>vocab_size<br ALIGN="LEFT"/>|forward(input_ids: torch.LongTensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.LongTensor], past_key_values: Optional[List[torch.FloatTensor]], inputs_embeds: Optional[torch.FloatTensor], labels: Optional[torch.LongTensor], use_cache: Optional[bool], output_attentions: Optional[bool], output_hidden_states: Optional[bool], return_dict: Optional[bool]): Union[Tuple, CausalLMOutputWithPast]<br ALIGN="LEFT"/>get_decoder()<br ALIGN="LEFT"/>get_input_embeddings()<br ALIGN="LEFT"/>get_output_embeddings()<br ALIGN="LEFT"/>prepare_inputs_for_generation(input_ids, past_key_values, attention_mask, inputs_embeds)<br ALIGN="LEFT"/>set_decoder(decoder)<br ALIGN="LEFT"/>set_input_embeddings(value)<br ALIGN="LEFT"/>set_output_embeddings(new_embeddings)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekForSequenceClassification" [color="black", fontcolor="black", label=<{DeepseekForSequenceClassification|model<br ALIGN="LEFT"/>num_labels<br ALIGN="LEFT"/>score : Linear<br ALIGN="LEFT"/>|forward(input_ids: torch.LongTensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.LongTensor], past_key_values: Optional[List[torch.FloatTensor]], inputs_embeds: Optional[torch.FloatTensor], labels: Optional[torch.LongTensor], use_cache: Optional[bool], output_attentions: Optional[bool], output_hidden_states: Optional[bool], return_dict: Optional[bool]): Union[Tuple, SequenceClassifierOutputWithPast]<br ALIGN="LEFT"/>get_input_embeddings()<br ALIGN="LEFT"/>set_input_embeddings(value)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekLinearScalingRotaryEmbedding" [color="black", fontcolor="black", label=<{DeepseekLinearScalingRotaryEmbedding|max_seq_len_cached<br ALIGN="LEFT"/>scaling_factor : float<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekMLP" [color="black", fontcolor="black", label=<{DeepseekMLP|act_fn<br ALIGN="LEFT"/>config<br ALIGN="LEFT"/>down_proj : Linear<br ALIGN="LEFT"/>gate_proj : Linear<br ALIGN="LEFT"/>hidden_size : NoneType<br ALIGN="LEFT"/>intermediate_size : NoneType<br ALIGN="LEFT"/>up_proj : Linear<br ALIGN="LEFT"/>|forward(x)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekMoE" [color="black", fontcolor="black", label=<{DeepseekMoE|config<br ALIGN="LEFT"/>experts : ModuleList<br ALIGN="LEFT"/>gate<br ALIGN="LEFT"/>num_experts_per_tok<br ALIGN="LEFT"/>shared_experts<br ALIGN="LEFT"/>|forward(hidden_states)<br ALIGN="LEFT"/>moe_infer(x, flat_expert_indices, flat_expert_weights)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekModel" [color="black", fontcolor="black", label=<{DeepseekModel|embed_tokens : Embedding<br ALIGN="LEFT"/>gradient_checkpointing : bool<br ALIGN="LEFT"/>layers : ModuleList<br ALIGN="LEFT"/>norm<br ALIGN="LEFT"/>padding_idx<br ALIGN="LEFT"/>vocab_size<br ALIGN="LEFT"/>|forward(input_ids: torch.LongTensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.LongTensor], past_key_values: Optional[List[torch.FloatTensor]], inputs_embeds: Optional[torch.FloatTensor], use_cache: Optional[bool], output_attentions: Optional[bool], output_hidden_states: Optional[bool], return_dict: Optional[bool]): Union[Tuple, BaseModelOutputWithPast]<br ALIGN="LEFT"/>get_input_embeddings()<br ALIGN="LEFT"/>set_input_embeddings(value)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekPreTrainedModel" [color="black", fontcolor="black", label=<{DeepseekPreTrainedModel|base_model_prefix : str<br ALIGN="LEFT"/>config_class<br ALIGN="LEFT"/>supports_gradient_checkpointing : bool<br ALIGN="LEFT"/>|}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekRMSNorm" [color="black", fontcolor="black", label=<{DeepseekRMSNorm|variance_epsilon : float<br ALIGN="LEFT"/>weight : Parameter<br ALIGN="LEFT"/>|forward(hidden_states)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekRotaryEmbedding" [color="black", fontcolor="black", label=<{DeepseekRotaryEmbedding|base : int<br ALIGN="LEFT"/>dim<br ALIGN="LEFT"/>max_position_embeddings : int<br ALIGN="LEFT"/>max_seq_len_cached : NoneType<br ALIGN="LEFT"/>|forward(x, seq_len)<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekSdpaAttention" [color="black", fontcolor="black", label=<{DeepseekSdpaAttention|<br ALIGN="LEFT"/>|forward(hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.LongTensor], past_key_value: Optional[Cache], output_attentions: bool, use_cache: bool): Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.MoEGate" [color="black", fontcolor="black", label=<{MoEGate|alpha<br ALIGN="LEFT"/>config<br ALIGN="LEFT"/>gating_dim<br ALIGN="LEFT"/>n_routed_experts<br ALIGN="LEFT"/>norm_topk_prob<br ALIGN="LEFT"/>scoring_func<br ALIGN="LEFT"/>seq_aux<br ALIGN="LEFT"/>top_k<br ALIGN="LEFT"/>weight : Parameter<br ALIGN="LEFT"/>|forward(hidden_states)<br ALIGN="LEFT"/>reset_parameters(): None<br ALIGN="LEFT"/>}>, shape="record", style="solid"];
"modeling_deepseek.DeepseekDynamicNTKScalingRotaryEmbedding" -> "modeling_deepseek.DeepseekRotaryEmbedding" [arrowhead="empty", arrowtail="none"];
"modeling_deepseek.DeepseekFlashAttention2" -> "modeling_deepseek.DeepseekAttention" [arrowhead="empty", arrowtail="none"];
"modeling_deepseek.DeepseekForCausalLM" -> "modeling_deepseek.DeepseekPreTrainedModel" [arrowhead="empty", arrowtail="none"];
"modeling_deepseek.DeepseekForSequenceClassification" -> "modeling_deepseek.DeepseekPreTrainedModel" [arrowhead="empty", arrowtail="none"];
"modeling_deepseek.DeepseekLinearScalingRotaryEmbedding" -> "modeling_deepseek.DeepseekRotaryEmbedding" [arrowhead="empty", arrowtail="none"];
"modeling_deepseek.DeepseekModel" -> "modeling_deepseek.DeepseekPreTrainedModel" [arrowhead="empty", arrowtail="none"];
"modeling_deepseek.DeepseekSdpaAttention" -> "modeling_deepseek.DeepseekAttention" [arrowhead="empty", arrowtail="none"];
"modeling_deepseek.DeepseekDynamicNTKScalingRotaryEmbedding" -> "modeling_deepseek.DeepseekAttention" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="rotary_emb", style="solid"];
"modeling_deepseek.DeepseekLinearScalingRotaryEmbedding" -> "modeling_deepseek.DeepseekAttention" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="rotary_emb", style="solid"];
"modeling_deepseek.DeepseekMLP" -> "modeling_deepseek.DeepseekDecoderLayer" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="mlp", style="solid"];
"modeling_deepseek.DeepseekMLP" -> "modeling_deepseek.DeepseekMoE" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="shared_experts", style="solid"];
"modeling_deepseek.DeepseekMoE" -> "modeling_deepseek.DeepseekDecoderLayer" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="mlp", style="solid"];
"modeling_deepseek.DeepseekModel" -> "modeling_deepseek.DeepseekForCausalLM" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="model", style="solid"];
"modeling_deepseek.DeepseekModel" -> "modeling_deepseek.DeepseekForSequenceClassification" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="model", style="solid"];
"modeling_deepseek.DeepseekRMSNorm" -> "modeling_deepseek.DeepseekDecoderLayer" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="input_layernorm", style="solid"];
"modeling_deepseek.DeepseekRMSNorm" -> "modeling_deepseek.DeepseekDecoderLayer" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="post_attention_layernorm", style="solid"];
"modeling_deepseek.DeepseekRMSNorm" -> "modeling_deepseek.DeepseekModel" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="norm", style="solid"];
"modeling_deepseek.DeepseekRotaryEmbedding" -> "modeling_deepseek.DeepseekAttention" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="rotary_emb", style="solid"];
"modeling_deepseek.MoEGate" -> "modeling_deepseek.DeepseekMoE" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="gate", style="solid"];
}
